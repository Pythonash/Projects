{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## NP\n\n1. Submit after turned on GPU.\n\n2. Clear the ubiquant-parquet dataset before submit.\n\n\n# 0217 로그\n\n조정할 파라미터:\n\n**Leaky relu**\n\n> 0.1\n\n**initial_learning_rate**\n\n> 0.004\n\n**epochs**\n\n> 100\n\n\n그리고 데이터셋 추가시켜놓고 돌렸어야 헀는데 못돌림... 일단 정리하면\n\n- 뉴런 수를 줄이는게 더 도움이 되었음\n\n- 모든피쳐를 다 사용하는게 더 도움이 되었음\n\n- Leaky relu 0.1로, 학습률은 0.003일때 0.149\n\nThe socre is 0.143.\n\n# 0218 로그\n\ngpu 키고 실행 누를것\n\n조정할 파라미터:\n\n**Leaky relu**\n\n> 0.2\n\n**initial_learning_rate**\n\n> 0.003\n\n**epochs**\n\n> 50\n\nThe score is 0.147.\n\n---\n\n조정할 파라미터:\n\n**Leaky relu**\n\n> 0.2\n\n**initial_learning_rate**\n\n> 0.0025\n\n**epochs**\n\n> 50\n\nThe score is 0.148.\n\n---\n\n조정할 파라미터:\n\n**Leaky relu**\n\n> 0.1\n\n**initial_learning_rate**\n\n> 0.0025\n\n**epochs**\n\n> 50\n\nThe score is 0.145.\n\n---\n\n조정할 파라미터:\n\n**Leaky relu**\n\n> 0.1\n\n**initial_learning_rate**\n\n> 0.0035\n\n**epochs**\n\n> 50\n\nThe score is 0.147.\n\n# 0219 로그\n\ngpu 키고 실행 누를것\n\n조정할 파라미터:\n\n**Leaky relu**\n\n> 0.2\n\n**initial_learning_rate**\n\n> 0.0035\n\n**epochs**\n\n> 50\n\nThe score is\n","metadata":{}},{"cell_type":"markdown","source":"# Experiment design result.\n\n---\n\n## When leaky relu value was 0.1.\n\n**0. learning_rate = 0.002**\n\n> 0.145\n\n**1. learning_rate = 0.0025**\n\n> 0.145\n\n**2. learning_rate = 0.003**\n\n> 0.149\n\n**3. learning_rate = 0.0035**\n\n> 0.147\n\n**4. learning_rate = 0.004**\n\n> 0.143\n\n**5. learning_rate = 0.003 & batch = 512**\n\n> 0.147\n\n\n---\n\n\n\n## When leaky relu value was 0.2.\n\n**0. learning_rate = 0.0015**\n\n> 0.148\n\n**0. learning_rate = 0.00175**\n\n> 0.147\n\n**1. learning_rate = 0.002**\n\n> 0.149\n\n**2. learning_rate = 0.0025**\n\n> 0.148\n\n**3. learning_rate = 0.003**\n\n> 0.147\n\n**4. learning_rate = 0.0035**\n\n> 0.146\n\n**5. learning_rate = 0.002 & batch = 512**\n\n> 0.147\n\n---\n\n\n**Leaky 0.1, Adamax learning_rate = 0.003, and batch = 1024**\n\n> 0.148\n\n**Leaky 0.2, Adamax learning_rate = 0.002, and batch = 1024**\n\n> 0.147\n\n**Leaky 0.1, Adamax learning_rate = 0.002, and batch = 1024**\n\n> 0.148\n\n**Leaky 0.2, Adamax learning_rate = 0.003, and batch = 1024**\n\n> 0.145\n\n**Leaky 0.05, Adam learning_rate = 0.003, and batch = 1024**\n\n> 0.150\n\n**Leaky 0.05, Adamax learning_rate = 0.003, and batch = 1024**\n\n> 0.147\n\n**Leaky 0.05, Adam learning_rate = 0.002, and batch = 1024**\n\n> 0.145\n\n**Leaky 0.05, Adam learning_rate = 0.0031, and batch = 1024**\n\n> 0.142\n\n**Leaky 0.05, Adam learning_rate = 0.0029, and batch = 1024**\n\n> 0.143\n\n**Leaky 0.03, Adam learning_rate = 0.003, and batch = 1024**\n\n> 0.145\n\n**Leaky 0.07, Adam learning_rate = 0.003, and batch = 1024**\n\n> 0.148\n\n**Leaky 0.05, Adam learning_rate = 0.003, Dropout = 0.5, and batch = 1024**\n\n> 0.142\n\n**Leaky 0.04, Adam learning_rate = 0.003, Dropout = 0.4, and batch = 1024**\n\n> 0.147\n\n**Leaky 0.1, Adam learning_rate = 0.003, Dropout = 0.4, decay_stpes = 100000, decay_rate = 0.96, and batch = 1024**\n\n> 0.149\n\n**Leaky 0.1, Adam learning_rate = 0.003, Dropout = 0.4, decay_stpes = 100000, decay_rate = 0.97, and batch = 1024**\n\n> 0.148\n\n**Leaky 0.1, Adam learning_rate = 0.003, Dropout = 0.4, decay_stpes = 100000, decay_rate = 0.98, and batch = 1024**\n\n> 0.147\n\n**Leaky 0.05, Adam learning_rate = 0.003, Dropout = 0.4, decay_stpes = 100000, decay_rate = 0.96, and batch = 1024**\n\n> 0.149\n\n**Leaky 0.05, Adam learning_rate = 0.003, Dropout = 0.4, decay_stpes = 100000, decay_rate = 0.97, and batch = 1024**\n\n> 0.147\n\n**Leaky 0.05, Adam learning_rate = 0.003, Dropout = 0.4, decay_stpes = 100000, decay_rate = 0.98, and batch = 1024**\n\n> 0.149\n\n**investment_id X leaky 0.05 Adam 0.003 deacy 9700 0.98**\n\n> 0.145\n\n---\n\n**Leaky 0.05, Adam learning_rate = 0.003, Dropout = 0.4, decay_stpes = 100000, decay_rate = 0.95, and batch = 1024**\n\n> 0.149\n\n**Leaky 0.1, Adam learning_rate = 0.003, Dropout = 0.4, decay_stpes = 100000, decay_rate = 0.95, and batch = 1024**\n\n> 0.145\n\n---\n**Leaky 0.05, Adam learning_rate = 0.003, Dropout = 0.4, decay_stpes = 100000, decay_rate = 0.94, and batch = 1024**\n\n> 0.146\n\n**Leaky 0.05, Adam learning_rate = 0.003, Dropout = 0.4, decay_stpes = 9800, decay_rate = 0.98, and batch = 32**\n\n> error\n\n**Leaky 0.05, Adam learning_rate = 0.003, Dropout = 0.4, decay_stpes = 100000, decay_rate = 0.98, and batch = 32**\n\n> error","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.preprocessing import StandardScaler\nimport ubiquant\nfrom sklearn.model_selection import KFold\n\ndf = pd.read_parquet('../input/ubiquant-parquet/train_low_mem.parquet')\nf_col = df.drop(['row_id','time_id','investment_id','target'],axis=1).columns\nscaler = StandardScaler()\nscaler.fit(pd.DataFrame(df['investment_id']))\ndef make_dataset(df):\n    inv_df = df['investment_id']\n    f_df = df[f_col]\n    scaled_investment_id = scaler.transform(pd.DataFrame(inv_df))\n    df['investment_id'] = scaled_investment_id\n    data_x = pd.concat([df['investment_id'], f_df], axis=1)\n    return data_x\n\ndf=df.astype('float16')\ndf_x = make_dataset(df)\ndf_y = pd.DataFrame(df['target'])\ndel df","metadata":{"papermill":{"duration":42.185531,"end_time":"2022-02-17T06:50:32.211548","exception":false,"start_time":"2022-02-17T06:49:50.026017","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-27T15:47:05.634014Z","iopub.execute_input":"2022-02-27T15:47:05.634316Z","iopub.status.idle":"2022-02-27T15:47:35.401144Z","shell.execute_reply.started":"2022-02-27T15:47:05.634265Z","shell.execute_reply":"2022-02-27T15:47:35.399439Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# def pythonash_model():\n#     inputs_ = tf.keras.Input(shape = [df_x.shape[1]])\n#     batch = tf.keras.layers.BatchNormalization()(inputs_)\n#     x = tf.keras.layers.Dense(32, kernel_initializer = 'he_normal')(batch)\n#     batch = tf.keras.layers.BatchNormalization()(x)\n#     leaky = tf.keras.layers.LeakyReLU(0.01)(batch)\n    \n#     x = tf.keras.layers.Dense(64, kernel_initializer = 'he_normal')(leaky)\n#     batch = tf.keras.layers.BatchNormalization()(x)\n#     leaky = tf.keras.layers.LeakyReLU(0.01)(batch)\n    \n#     x = tf.keras.layers.Dense(128, kernel_initializer = 'he_normal')(leaky)\n#     batch = tf.keras.layers.BatchNormalization()(x)\n#     leaky = tf.keras.layers.LeakyReLU(0.01)(batch)\n    \n#     x = tf.keras.layers.Dense(256, kernel_initializer = 'he_normal')(leaky)\n#     batch = tf.keras.layers.BatchNormalization()(x)\n#     leaky = tf.keras.layers.LeakyReLU(0.01)(batch)\n    \n#     x = tf.keras.layers.Dense(512, kernel_initializer = 'he_normal')(leaky)\n#     batch = tf.keras.layers.BatchNormalization()(x)\n#     leaky = tf.keras.layers.LeakyReLU(0.01)(batch)\n# #     drop = tf.keras.layers.Dropout(0.4)(leaky)\n    \n#     x = tf.keras.layers.Dense(256, kernel_initializer = 'he_normal')(leaky)\n#     batch = tf.keras.layers.BatchNormalization()(x)\n#     leaky = tf.keras.layers.LeakyReLU(0.01)(batch)\n\n    \n#     x = tf.keras.layers.Dense(128, kernel_initializer = 'he_normal')(leaky)\n#     batch = tf.keras.layers.BatchNormalization()(x)\n#     leaky = tf.keras.layers.LeakyReLU(0.01)(batch)\n    \n#     x = tf.keras.layers.Dense(64, kernel_initializer = 'he_normal')(leaky)\n#     batch = tf.keras.layers.BatchNormalization()(x)\n#     leaky = tf.keras.layers.LeakyReLU(0.01)(batch)\n    \n#     x = tf.keras.layers.Dense(32, kernel_initializer = 'he_normal')(leaky)\n#     batch = tf.keras.layers.BatchNormalization()(x)\n#     leaky = tf.keras.layers.LeakyReLU(0.01)(batch)\n    \n#     x = tf.keras.layers.Dense(16, kernel_initializer = 'he_normal')(leaky)\n#     batch = tf.keras.layers.BatchNormalization()(x)\n#     leaky = tf.keras.layers.LeakyReLU(0.01)(batch)\n    \n#     x = tf.keras.layers.Dense(8, kernel_initializer = 'he_normal')(leaky)\n#     batch = tf.keras.layers.BatchNormalization()(x)\n#     leaky = tf.keras.layers.LeakyReLU(0.01)(batch)\n# #     drop = tf.keras.layers.Dropout(0.4)(leaky)\n    \n    \n#     outputs_ = tf.keras.layers.Dense(1)(leaky)\n    \n#     model = tf.keras.Model(inputs = inputs_, outputs = outputs_)\n    \n#     rmse = tf.keras.metrics.RootMeanSquaredError()\n\n#     learning_sch = tf.keras.optimizers.schedules.ExponentialDecay(\n#     initial_learning_rate = 0.003,\n#     decay_steps = 50000,\n#     decay_rate = 0.97)\n#     adam = tf.keras.optimizers.Adam(learning_rate = learning_sch)\n    \n#     model.compile(loss = 'mse', metrics = rmse, optimizer = adam)\n#     return model\n","metadata":{"papermill":{"duration":3.3428,"end_time":"2022-02-17T06:50:53.606125","exception":false,"start_time":"2022-02-17T06:50:50.263325","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-27T15:47:35.408304Z","iopub.execute_input":"2022-02-27T15:47:35.408632Z","iopub.status.idle":"2022-02-27T15:47:35.414652Z","shell.execute_reply.started":"2022-02-27T15:47:35.408589Z","shell.execute_reply":"2022-02-27T15:47:35.413614Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def pythonash_model():\n    inputs_ = tf.keras.Input(shape = [df_x.shape[1]])\n    x = tf.keras.layers.Dense(64, kernel_initializer = 'he_normal')(inputs_)\n    batch = tf.keras.layers.BatchNormalization()(x)\n#    leaky = tf.keras.layers.LeakyReLU(0.02)(batch)\n#    leaky = tf.keras.layers.Activation('selu')(batch)\n    leaky = tf.keras.layers.PReLU()(batch)\n    \n    x = tf.keras.layers.Dense(128, kernel_initializer = 'he_normal')(leaky)\n    batch = tf.keras.layers.BatchNormalization()(x)\n#    leaky = tf.keras.layers.Activation('selu')(batch)\n    leaky = tf.keras.layers.PReLU()(batch)\n    \n    x = tf.keras.layers.Dense(256, kernel_initializer = 'he_normal')(leaky)\n    batch = tf.keras.layers.BatchNormalization()(x)\n #   leaky = tf.keras.layers.Activation('selu')(batch)\n    leaky = tf.keras.layers.PReLU()(batch)\n    \n    x = tf.keras.layers.Dense(512, kernel_initializer = 'he_normal')(leaky)\n    batch = tf.keras.layers.BatchNormalization()(x)\n#    leaky = tf.keras.layers.Activation('selu')(batch)\n    leaky = tf.keras.layers.PReLU()(batch)\n    \n    x = tf.keras.layers.Dense(256, kernel_initializer = 'he_normal')(leaky)\n    batch = tf.keras.layers.BatchNormalization()(x)\n#    leaky = tf.keras.layers.Activation('selu')(batch)\n    leaky = tf.keras.layers.PReLU()(batch)\n    drop = tf.keras.layers.Dropout(0.4)(leaky)\n    \n    x = tf.keras.layers.Dense(128, kernel_initializer = 'he_normal')(drop)\n    batch = tf.keras.layers.BatchNormalization()(x)\n#    leaky = tf.keras.layers.Activation('selu')(batch)\n    leaky = tf.keras.layers.PReLU()(batch)\n    \n    x = tf.keras.layers.Dense(8, kernel_initializer = 'he_normal')(leaky)\n    batch = tf.keras.layers.BatchNormalization()(x)\n#    leaky = tf.keras.layers.Activation('selu')(batch)\n    leaky = tf.keras.layers.PReLU()(batch)\n    drop = tf.keras.layers.Dropout(0.4)(leaky)\n    \n    outputs_ = tf.keras.layers.Dense(1)(drop)\n    \n    model = tf.keras.Model(inputs = inputs_, outputs = outputs_)\n    \n    rmse = tf.keras.metrics.RootMeanSquaredError()\n\n#     learning_sch = tf.keras.optimizers.schedules.ExponentialDecay(\n#     initial_learning_rate = 0.03,\n#     decay_steps = 9700,\n#     decay_rate = 0.98)\n    adam = tf.keras.optimizers.Adam(learning_rate = 0.03)\n    \n    model.compile(loss = 'mse', metrics = rmse, optimizer = adam)\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-27T15:47:35.415985Z","iopub.execute_input":"2022-02-27T15:47:35.416374Z","iopub.status.idle":"2022-02-27T15:47:35.433582Z","shell.execute_reply.started":"2022-02-27T15:47:35.416338Z","shell.execute_reply":"2022-02-27T15:47:35.432890Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"kfold_generator = KFold(n_splits =5, shuffle=True, random_state = 2022)\nkfold_generator","metadata":{"papermill":{"duration":0.046542,"end_time":"2022-02-17T06:50:55.41161","exception":false,"start_time":"2022-02-17T06:50:55.365068","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-27T15:47:35.435795Z","iopub.execute_input":"2022-02-27T15:47:35.436255Z","iopub.status.idle":"2022-02-27T15:47:35.447402Z","shell.execute_reply.started":"2022-02-27T15:47:35.436217Z","shell.execute_reply":"2022-02-27T15:47:35.446728Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Write your model name down in 'pythonash_model.h5'.\ncallbacks = tf.keras.callbacks.ModelCheckpoint('pythonash_model.h5', save_best_only = True)\nfor train_index, val_index in kfold_generator.split(df_x, df_y):\n    # Split training dataset.\n    train_x, train_y = df_x.iloc[train_index], df_y.iloc[train_index]\n    # Split validation dataset.\n    val_x, val_y = df_x.iloc[val_index], df_y.iloc[val_index]\n    # Make tensor dataset.\n    tf_train = tf.data.Dataset.from_tensor_slices((train_x, train_y)).shuffle(2022).batch(1024, drop_remainder=True).prefetch(1)\n    tf_val = tf.data.Dataset.from_tensor_slices((val_x, val_y)).shuffle(2022).batch(1024, drop_remainder=True).prefetch(1)\n    # Load model\n    model = pythonash_model()\n    # Model fitting\n    \n    ## I used 5 epochs for fast save.\n    ## Change the epochs into more numbers.\n    model.fit(tf_train, callbacks = callbacks, epochs = 5, #### change the epochs into more numbers.\n             validation_data = (tf_val), shuffle=True)\n    # Delete tensor dataset and model for avoiding memory exploring.\n    del tf_train\n    del tf_val\n    del model\n    \n    \n    \ndef scheduling_model(lr):\n    model = tf.keras.models.load_model('pythonash_model.h5')\n    adam = tf.keras.optimizers.Adam(learning_rate = lr)\n    rmse = tf.keras.metrics.RootMeanSquaredError()\n    model.compile(loss = 'mse', metrics = rmse, optimizer = adam)\n    return model\n\n\nfor train_index, val_index in kfold_generator.split(df_x, df_y):\n    # Split training dataset.\n    train_x, train_y = df_x.iloc[train_index], df_y.iloc[train_index]\n    # Split validation dataset.\n    val_x, val_y = df_x.iloc[val_index], df_y.iloc[val_index]\n    # Make tensor dataset.\n    tf_train = tf.data.Dataset.from_tensor_slices((train_x, train_y)).shuffle(2022).batch(1024, drop_remainder=True).prefetch(1)\n    tf_val = tf.data.Dataset.from_tensor_slices((val_x, val_y)).shuffle(2022).batch(1024, drop_remainder=True).prefetch(1)\n    # Load model\n    model = scheduling_model(0.01)\n    # Model fitting\n    \n    ## I used 5 epochs for fast save.\n    ## Change the epochs into more numbers.\n    model.fit(tf_train, callbacks = callbacks, epochs = 5, #### change the epochs into more numbers.\n             validation_data = (tf_val), shuffle=True)\n    # Delete tensor dataset and model for avoiding memory exploring.\n    del tf_train\n    del tf_val\n    del model\n\nfor train_index, val_index in kfold_generator.split(df_x, df_y):\n    # Split training dataset.\n    train_x, train_y = df_x.iloc[train_index], df_y.iloc[train_index]\n    # Split validation dataset.\n    val_x, val_y = df_x.iloc[val_index], df_y.iloc[val_index]\n    # Make tensor dataset.\n    tf_train = tf.data.Dataset.from_tensor_slices((train_x, train_y)).shuffle(2022).batch(1024, drop_remainder=True).prefetch(1)\n    tf_val = tf.data.Dataset.from_tensor_slices((val_x, val_y)).shuffle(2022).batch(1024, drop_remainder=True).prefetch(1)\n    # Load model\n    model = scheduling_model(0.005)\n    # Model fitting\n    \n    ## I used 5 epochs for fast save.\n    ## Change the epochs into more numbers.\n    model.fit(tf_train, callbacks = callbacks, epochs = 5, #### change the epochs into more numbers.\n             validation_data = (tf_val), shuffle=True)\n    # Delete tensor dataset and model for avoiding memory exploring.\n    del tf_train\n    del tf_val\n    del model\n\nfor train_index, val_index in kfold_generator.split(df_x, df_y):\n    # Split training dataset.\n    train_x, train_y = df_x.iloc[train_index], df_y.iloc[train_index]\n    # Split validation dataset.\n    val_x, val_y = df_x.iloc[val_index], df_y.iloc[val_index]\n    # Make tensor dataset.\n    tf_train = tf.data.Dataset.from_tensor_slices((train_x, train_y)).shuffle(2022).batch(1024, drop_remainder=True).prefetch(1)\n    tf_val = tf.data.Dataset.from_tensor_slices((val_x, val_y)).shuffle(2022).batch(1024, drop_remainder=True).prefetch(1)\n    # Load model\n    model = scheduling_model(0.003)\n    # Model fitting\n    \n    ## I used 5 epochs for fast save.\n    ## Change the epochs into more numbers.\n    model.fit(tf_train, callbacks = callbacks, epochs = 5, #### change the epochs into more numbers.\n             validation_data = (tf_val), shuffle=True)\n    # Delete tensor dataset and model for avoiding memory exploring.\n    del tf_train\n    del tf_val\n    del model\n\n####### Fine tuning #################\n\n\n","metadata":{"papermill":{"duration":568.551778,"end_time":"2022-02-17T07:00:24.07599","exception":false,"start_time":"2022-02-17T06:50:55.524212","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-27T15:47:35.448985Z","iopub.execute_input":"2022-02-27T15:47:35.449671Z","iopub.status.idle":"2022-02-27T16:37:33.806528Z","shell.execute_reply.started":"2022-02-27T15:47:35.449634Z","shell.execute_reply":"2022-02-27T16:37:33.805766Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def tuned_model_freezing(lr):\n    model = tf.keras.models.load_model('pythonash_model.h5')\n    for layer in model.layers:\n        layer.trainable = False\n    x = tf.keras.layers.Dense(4, kernel_initializer = 'he_normal')(model.layers[-2].output)\n    batch = tf.keras.layers.BatchNormalization()(x)\n    leaky = tf.keras.layers.PReLU()(batch)\n    x = tf.keras.layers.Dense(2, kernel_initializer = 'he_normal')(leaky)\n    batch = tf.keras.layers.BatchNormalization()(x)\n    leaky = tf.keras.layers.PReLU()(batch)\n    x = tf.keras.layers.Dense(1)(leaky)\n    model = tf.keras.Model(inputs = model.input, outputs = x)\n    adam = tf.keras.optimizers.Adam(learning_rate = lr)\n    rmse = tf.keras.metrics.RootMeanSquaredError()\n    model.compile(loss = 'mse', metrics = rmse, optimizer = adam)\n    return model\n\n\n\n\ncallbacks = tf.keras.callbacks.ModelCheckpoint('pythonash_model_freezing.h5', save_best_only = True)\nfor train_index, val_index in kfold_generator.split(df_x, df_y):\n    # Split training dataset.\n    train_x, train_y = df_x.iloc[train_index], df_y.iloc[train_index]\n    # Split validation dataset.\n    val_x, val_y = df_x.iloc[val_index], df_y.iloc[val_index]\n    # Make tensor dataset.\n    tf_train = tf.data.Dataset.from_tensor_slices((train_x, train_y)).shuffle(2022).batch(1024, drop_remainder=True).prefetch(1)\n    tf_val = tf.data.Dataset.from_tensor_slices((val_x, val_y)).shuffle(2022).batch(1024, drop_remainder=True).prefetch(1)\n    # Load model\n    model = tuned_model_freezing(0.03)\n    # Model fitting\n    \n    ## I used 5 epochs for fast save.\n    ## Change the epochs into more numbers.\n    model.fit(tf_train, callbacks = callbacks, epochs = 5, #### change the epochs into more numbers.\n             validation_data = (tf_val), shuffle=True)\n    # Delete tensor dataset and model for avoiding memory exploring.\n    del tf_train\n    del tf_val\n    del model\n    \n    \n    \n    \ndef scheduling_model(lr):\n    model = tf.keras.models.load_model('pythonash_model_freezing.h5')\n    adam = tf.keras.optimizers.Adam(learning_rate = lr)\n    rmse = tf.keras.metrics.RootMeanSquaredError()\n    model.compile(loss = 'mse', metrics = rmse, optimizer = adam)\n    return model    \n    \n\nfor train_index, val_index in kfold_generator.split(df_x, df_y):\n    # Split training dataset.\n    train_x, train_y = df_x.iloc[train_index], df_y.iloc[train_index]\n    # Split validation dataset.\n    val_x, val_y = df_x.iloc[val_index], df_y.iloc[val_index]\n    # Make tensor dataset.\n    tf_train = tf.data.Dataset.from_tensor_slices((train_x, train_y)).shuffle(2022).batch(1024, drop_remainder=True).prefetch(1)\n    tf_val = tf.data.Dataset.from_tensor_slices((val_x, val_y)).shuffle(2022).batch(1024, drop_remainder=True).prefetch(1)\n    # Load model\n    model = scheduling_model(0.01)\n    # Model fitting\n    \n    ## I used 5 epochs for fast save.\n    ## Change the epochs into more numbers.\n    model.fit(tf_train, callbacks = callbacks, epochs = 5, #### change the epochs into more numbers.\n             validation_data = (tf_val), shuffle=True)\n    # Delete tensor dataset and model for avoiding memory exploring.\n    del tf_train\n    del tf_val\n    del model    \n    \n    \n    \n    \n    \n    \ndef tuned_model_unfreezing(lr):\n    model = tf.keras.models.load_model('pythonash_model_freezing.h5')\n    for layer in model.layers:\n        layer.trainable = True\n    adam = tf.keras.optimizers.Adam(learning_rate = lr)\n    rmse = tf.keras.metrics.RootMeanSquaredError()\n    model.compile(loss = 'mse', metrics = rmse, optimizer = adam)\n    return model\n\ncallbacks = tf.keras.callbacks.ModelCheckpoint('pythonash_model_unfreezing.h5', save_best_only = True)\nfor train_index, val_index in kfold_generator.split(df_x, df_y):\n    # Split training dataset.\n    train_x, train_y = df_x.iloc[train_index], df_y.iloc[train_index]\n    # Split validation dataset.\n    val_x, val_y = df_x.iloc[val_index], df_y.iloc[val_index]\n    # Make tensor dataset.\n    tf_train = tf.data.Dataset.from_tensor_slices((train_x, train_y)).shuffle(2022).batch(1024, drop_remainder=True).prefetch(1)\n    tf_val = tf.data.Dataset.from_tensor_slices((val_x, val_y)).shuffle(2022).batch(1024, drop_remainder=True).prefetch(1)\n    # Load model\n    model = tuned_model_unfreezing(0.003)\n    # Model fitting\n    \n    ## I used 5 epochs for fast save.\n    ## Change the epochs into more numbers.\n    model.fit(tf_train, callbacks = callbacks, epochs = 5, #### change the epochs into more numbers.\n             validation_data = (tf_val), shuffle=True)\n    # Delete tensor dataset and model for avoiding memory exploring.\n    del tf_train\n    del tf_val\n    del model\n    \n    \ndef scheduling_model(lr):\n    model = tf.keras.models.load_model('pythonash_model_unfreezing.h5')\n    adam = tf.keras.optimizers.Adam(learning_rate = lr)\n    rmse = tf.keras.metrics.RootMeanSquaredError()\n    model.compile(loss = 'mse', metrics = rmse, optimizer = adam)\n    return model    \n    \n\nfor train_index, val_index in kfold_generator.split(df_x, df_y):\n    # Split training dataset.\n    train_x, train_y = df_x.iloc[train_index], df_y.iloc[train_index]\n    # Split validation dataset.\n    val_x, val_y = df_x.iloc[val_index], df_y.iloc[val_index]\n    # Make tensor dataset.\n    tf_train = tf.data.Dataset.from_tensor_slices((train_x, train_y)).shuffle(2022).batch(1024, drop_remainder=True).prefetch(1)\n    tf_val = tf.data.Dataset.from_tensor_slices((val_x, val_y)).shuffle(2022).batch(1024, drop_remainder=True).prefetch(1)\n    # Load model\n    model = scheduling_model(0.001)\n    # Model fitting\n    \n    ## I used 5 epochs for fast save.\n    ## Change the epochs into more numbers.\n    model.fit(tf_train, callbacks = callbacks, epochs = 5, #### change the epochs into more numbers.\n             validation_data = (tf_val), shuffle=True)\n    # Delete tensor dataset and model for avoiding memory exploring.\n    del tf_train\n    del tf_val\n    del model ","metadata":{"execution":{"iopub.status.busy":"2022-02-27T16:37:33.808296Z","iopub.execute_input":"2022-02-27T16:37:33.808588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model = tf.keras.models.load_model('pythonash_model_unfreezing.h5')\nenv = ubiquant.make_env()   \niter_test = env.iter_test()    \nfor (test_df, sample_prediction_df) in iter_test:\n    test_df = make_dataset(test_df)\n    sample_prediction_df['target'] = best_model.predict(test_df)  \n    env.predict(sample_prediction_df)","metadata":{"papermill":{"duration":2.859981,"end_time":"2022-02-17T07:00:33.793431","exception":false,"start_time":"2022-02-17T07:00:30.93345","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}